{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import librosa\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import librosa.display\n",
    "#import keras\n",
    "#import datetime\n",
    "#import seaborn as sns\n",
    "#from sklearn.preprocessing import LabelEncoder \n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "#plt.rcParams['font.family'] = 'sans-serif' \n",
    "#plt.rcParams['font.serif'] = 'Ubuntu' \n",
    "#plt.rcParams['font.monospace'] = 'Ubuntu Mono' \n",
    "#plt.rcParams['font.size'] = 14 \n",
    "#plt.rcParams['axes.labelsize'] = 12 \n",
    "#plt.rcParams['axes.labelweight'] = 'bold' \n",
    "#plt.rcParams['axes.titlesize'] = 12 \n",
    "#plt.rcParams['xtick.labelsize'] = 12 \n",
    "#plt.rcParams['ytick.labelsize'] = 12 \n",
    "#plt.rcParams['legend.fontsize'] = 12 \n",
    "#plt.rcParams['figure.titlesize'] = 12 \n",
    "#plt.rcParams['image.cmap'] = 'jet' \n",
    "#plt.rcParams['image.interpolation'] = 'none' \n",
    "#plt.rcParams['figure.figsize'] = (10, 10) \n",
    "#plt.rcParams['axes.grid']=False\n",
    "#plt.rcParams['lines.linewidth'] = 2 \n",
    "#plt.rcParams['lines.markersize'] = 8\n",
    "#colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',\n",
    "#'xkcd:scarlet']\n",
    "#bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=colors[0], alpha=.5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y,sr = librosa.load('audios/Speaker26_000.wav')\n",
    "# dur_time = librosa.get_duration(y=y,sr=sr)\n",
    "#print(dur_time)\n",
    "#time_sec = round(dur_time)\n",
    "#S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=20, fmax=8000)\n",
    "#fig, ax = plt.subplots()\n",
    "#S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "#img = librosa.display.specshow(S_dB, x_axis='time',y_axis='mel', sr=sr,fmax=8000, ax=ax)\n",
    "#fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "#ax.set(title='Mel-frequency spectrogram')\n",
    "#plt.xlim()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_series_sec=np.linspace(0, time_sec, len(S_dB[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this section is the CSV files with the data to be handle by the model\n",
    "\n",
    "```\n",
    "trainData     : audio/train \n",
    "testData      : audio/test\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the names of the CSV files\n",
    "TRAIN_CSV_FILE = \"train.csv\"\n",
    "TEST_CSV_FILE = \"test.csv\"\n",
    "final_test=\"final_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the files in the folder audios/audios/train will be saved to train.csv\n",
      "CSV Header:  ['filename', 'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3263/3263 [18:42<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of extractWavFeatures\n",
      "The features of the files in the folder audios/audios/test will be saved to test.csv\n",
      "CSV Header:  ['filename', 'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 838/838 [11:50<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of extractWavFeatures\n",
      "CSV files are created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import librosa\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def extractWavFeatures(soundFilesFolder, csvFileName):\n",
    "    print(\"The features of the files in the folder \"+soundFilesFolder+\" will be saved to \"+csvFileName)\n",
    "    header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "    for i in range(1, 21):\n",
    "        header += f' mfcc{i}'\n",
    "    header += ' label'\n",
    "    header = header.split()\n",
    "    print('CSV Header: ', header)\n",
    "    file = open(csvFileName, 'w', newline='')\n",
    "    #with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for filename in tqdm(os.listdir(soundFilesFolder)):\n",
    "        number = f'{soundFilesFolder}/{filename}'\n",
    "        y, sr = librosa.load(number, mono=True, duration=30)\n",
    "        # remove leading and trailing silence\n",
    "        y, index = librosa.effects.trim(y)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        rmse = librosa.feature.rms(y=y)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'\n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        writer.writerow(to_append.split())\n",
    "    file.close()\n",
    "    print(\"End of extractWavFeatures\")\n",
    "    \n",
    "extractWavFeatures(\"audios/audios/train\", TRAIN_CSV_FILE)\n",
    "extractWavFeatures(\"audios/audios/test\", TEST_CSV_FILE)\n",
    "print(\"CSV files are created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv will be preprocessed\n",
      "Preprocessing is finished\n",
      "       rmse  spectral_centroid  spectral_bandwidth      rolloff  \\\n",
      "0  0.044474        2925.986137         2426.865665  5365.750887   \n",
      "1  0.007040        1054.481600          969.009484  2158.703613   \n",
      "2  0.059307         644.962153          689.094968  1064.355469   \n",
      "3  0.033733        1679.723927         1658.148768  3240.135427   \n",
      "4  0.034633        1303.101750         1210.428356  2418.177044   \n",
      "\n",
      "   zero_crossing_rate       mfcc1       mfcc2      mfcc3      mfcc4  \\\n",
      "0            0.170812 -292.403381   69.789474 -19.889078   4.436610   \n",
      "1            0.055127 -541.293884  209.900482 -59.150311  35.763649   \n",
      "2            0.034738 -379.739685  229.126648 -13.465721   7.315946   \n",
      "3            0.116656 -384.844421  104.027542  -0.869874  44.156509   \n",
      "4            0.083430 -420.164368  131.315445   6.856723  27.124081   \n",
      "\n",
      "       mfcc5  ...     mfcc12     mfcc13    mfcc14     mfcc15     mfcc16  \\\n",
      "0  -3.878308  ...  -2.387268  -6.745807 -3.034384 -10.344191  -5.455682   \n",
      "1  52.395588  ... -25.589903   1.971027 -9.283678  -8.496454  14.085022   \n",
      "2  10.072659  ... -15.177598 -10.289010 -4.679581  -8.963344  -4.572488   \n",
      "3  -0.136879  ...  -1.787667   4.390304 -9.613209  10.855107  -9.326888   \n",
      "4  10.203892  ...  -7.685796   0.104468 -8.183334  -0.137333  -2.409598   \n",
      "\n",
      "      mfcc17     mfcc18    mfcc19     mfcc20  number  \n",
      "0 -11.163910  -3.516300 -6.593322  -8.111452      50  \n",
      "1  -7.112857 -12.288487  4.079620 -15.449945      22  \n",
      "2  -7.788446 -10.445480 -3.889926 -11.179747      54  \n",
      "3  -0.155693  -4.752890 -5.889289   0.593901      13  \n",
      "4  -7.096424 -10.465993 -2.762266  -3.266744      25  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "test.csv will be preprocessed\n",
      "Preprocessing is finished\n",
      "       rmse  spectral_centroid  spectral_bandwidth      rolloff  \\\n",
      "0  0.062465        2494.766120         2259.537827  4510.464393   \n",
      "1  0.075160         843.041838          880.641004  1648.486328   \n",
      "2  0.021818        1738.811781         1724.522303  3465.993488   \n",
      "3  0.036983        3389.109496         2167.250752  5731.373677   \n",
      "4  0.055204        1734.297265         1426.316152  3041.781607   \n",
      "\n",
      "   zero_crossing_rate       mfcc1       mfcc2      mfcc3      mfcc4  \\\n",
      "0            0.155642 -315.935822   87.359421  16.839615  21.476034   \n",
      "1            0.043927 -319.378571  224.693375 -44.708931  15.121886   \n",
      "2            0.120636 -435.613434   97.572189  -7.425648  44.488117   \n",
      "3            0.226041 -279.897552   34.897182 -24.824188  28.362253   \n",
      "4            0.121150 -324.571442  102.503967 -21.985764  41.096210   \n",
      "\n",
      "       mfcc5  ...     mfcc12    mfcc13     mfcc14     mfcc15     mfcc16  \\\n",
      "0   3.381309  ...  -3.350798 -7.463634   3.006976  -1.351482  -7.327713   \n",
      "1  22.486296  ... -26.435188 -8.066481   2.414000 -10.282760  -7.912687   \n",
      "2   2.806721  ...  -2.122698  3.127735  -7.236073  11.505426 -12.148650   \n",
      "3 -19.190653  ...   0.659389 -8.243056   7.187273  -7.465928   2.551009   \n",
      "4  -9.374266  ...  -7.172763 -0.444135 -11.805051   2.977448  -3.675577   \n",
      "\n",
      "     mfcc17    mfcc18    mfcc19     mfcc20  number  \n",
      "0  3.380064 -5.589298 -3.320220  -2.090706      32  \n",
      "1 -8.371008 -5.420183 -0.954234 -11.587983      54  \n",
      "2  1.264230 -5.840943 -3.782394  -1.643455      13  \n",
      "3 -5.976261  6.004973 -9.561545   3.269066      48  \n",
      "4 -5.938468 -0.991449 -5.536480  -4.147708       1  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def preProcessData(csvFileName):\n",
    "    print(csvFileName+ \" will be preprocessed\")\n",
    "    data = pd.read_csv(csvFileName)\n",
    "    filenameArray = data['filename'] \n",
    "    speakerArray = []\n",
    "    #print(filenameArray)\n",
    "    for i in range(len(filenameArray)):\n",
    "        speaker = int(filenameArray[i].split(\"_\")[0].split(\"r\")[1])\n",
    "        #print(speaker)\n",
    "        #print(speaker)\n",
    "        speakerArray.append(speaker)\n",
    "    data['number'] = speakerArray\n",
    "    #Dropping unnecessary columns\n",
    "    data = data.drop(['filename'],axis=1)\n",
    "    data = data.drop(['label'],axis=1)\n",
    "    data = data.drop(['chroma_stft'],axis=1)\n",
    "    data.shape\n",
    "\n",
    "    print(\"Preprocessing is finished\")\n",
    "    print(data.head())\n",
    "    return data\n",
    "\n",
    "trainData = preProcessData(TRAIN_CSV_FILE)\n",
    "testData = preProcessData(TEST_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y from training data: (2284,)\n",
      "Y from validation data: (979,)\n",
      "Y from test data: (838,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(trainData.iloc[:, :-1], dtype = float)\n",
    "y = trainData.iloc[:, -1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n",
    "\n",
    "X_test = np.array(testData.iloc[:, :-1], dtype = float)\n",
    "y_test = testData.iloc[:, -1]\n",
    "\n",
    "print(\"Y from training data:\", y_train.shape)\n",
    "print(\"Y from validation data:\", y_val.shape)\n",
    "print(\"Y from test data:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X from training data (2284, 25)\n",
      "X from validation data (979, 25)\n",
      "X from test data (838, 25)\n"
     ]
    }
   ],
   "source": [
    "#Normalizing the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform( X_train )\n",
    "X_val = scaler.transform( X_val )\n",
    "X_test = scaler.transform( X_test )\n",
    "\n",
    "print(\"X from training data\", X_train.shape)\n",
    "print(\"X from validation data\", X_val.shape)\n",
    "print(\"X from test data\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of values: 3207\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55])\n",
      "{56}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import collections\n",
    "weight={}\n",
    "path='/home/silvia/Escritorio/TFM/audios/audios/56_speakers_audio_data'\n",
    "for speaker in os.listdir(path):\n",
    "    if speaker.find(\".sh\")==-1:\n",
    "        dir_path=path+'/'+speaker\n",
    "        speaker = int(speaker.split(\"_\")[0].split(\"r\")[1])\n",
    "        count = int(len(fnmatch.filter(os.listdir(dir_path), '*.*'))*0.8)-1\n",
    "        weight.update({speaker: count})\n",
    "max_value=sum(weight.values())\n",
    "print(\"Sum of values: \"+str(max_value))\n",
    "weight = {key: value for key, value in sorted(weight.items())}\n",
    "weight = {k: 1-(v/max_value) for k, v in weight.items()}\n",
    "print(weight.keys())\n",
    "print(set(range(57)) - set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 18:07:40.371949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-12 18:07:40.648213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-12 18:07:40.648226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-12 18:07:40.680501: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-12 18:07:41.500653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-12 18:07:41.500720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-12 18:07:41.500727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88544207  0.95914566  1.35700012 ...  0.04512399 -0.85655957\n",
      "   0.23008244]\n",
      " [ 0.1375018   0.14474688 -0.25622855 ...  0.89206799  0.55928048\n",
      "   1.56915991]\n",
      " [-0.36464669  0.80615091  1.2139867  ...  1.12698596  0.06999135\n",
      "   1.56905986]\n",
      " ...\n",
      " [ 2.10581517 -1.40915506 -1.3315279  ... -1.08303998  0.21637702\n",
      "  -0.18659736]\n",
      " [-0.21512721 -1.03718492 -0.64017014 ... -0.27088757  1.59214804\n",
      "  -0.00528993]\n",
      " [-0.43579081  1.40230586  1.71954114 ...  1.38549428 -0.52340609\n",
      "   1.32009819]]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 18:07:42.457207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-12 18:07:42.457406: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-12 18:07:42.457423: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (silvia-XPS-15-9500): /proc/driver/nvidia/version does not exist\n",
      "2022-09-12 18:07:42.457805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 11ms/step - loss: 4.2676 - accuracy: 0.0482 - val_loss: 4.0740 - val_accuracy: 0.2196\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6135 - accuracy: 0.1646 - val_loss: 2.8833 - val_accuracy: 0.3299\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0346 - accuracy: 0.2535 - val_loss: 2.3507 - val_accuracy: 0.4709\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.6810 - accuracy: 0.3214 - val_loss: 2.1069 - val_accuracy: 0.4954\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.4516 - accuracy: 0.3625 - val_loss: 1.9053 - val_accuracy: 0.5128\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.2823 - accuracy: 0.3975 - val_loss: 1.7260 - val_accuracy: 0.5700\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.0940 - accuracy: 0.4295 - val_loss: 1.5325 - val_accuracy: 0.6323\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.9408 - accuracy: 0.4623 - val_loss: 1.3759 - val_accuracy: 0.6772\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.8317 - accuracy: 0.4663 - val_loss: 1.2202 - val_accuracy: 0.7191\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.7150 - accuracy: 0.4987 - val_loss: 1.1005 - val_accuracy: 0.7630\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.6390 - accuracy: 0.5158 - val_loss: 0.9936 - val_accuracy: 0.7855\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.5447 - accuracy: 0.5385 - val_loss: 0.9063 - val_accuracy: 0.8274\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.4524 - accuracy: 0.5591 - val_loss: 0.8194 - val_accuracy: 0.8304\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.3633 - accuracy: 0.5849 - val_loss: 0.7364 - val_accuracy: 0.8447\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.3041 - accuracy: 0.5959 - val_loss: 0.6725 - val_accuracy: 0.8631\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.2489 - accuracy: 0.6060 - val_loss: 0.6135 - val_accuracy: 0.8907\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.1670 - accuracy: 0.6305 - val_loss: 0.5680 - val_accuracy: 0.8897\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.1069 - accuracy: 0.6445 - val_loss: 0.5164 - val_accuracy: 0.8999\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0846 - accuracy: 0.6541 - val_loss: 0.4718 - val_accuracy: 0.9234\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0328 - accuracy: 0.6624 - val_loss: 0.4393 - val_accuracy: 0.9203\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0034 - accuracy: 0.6791 - val_loss: 0.4049 - val_accuracy: 0.9244\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.9398 - accuracy: 0.6900 - val_loss: 0.3823 - val_accuracy: 0.9265\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.9552 - accuracy: 0.6922 - val_loss: 0.3652 - val_accuracy: 0.9387\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.8681 - accuracy: 0.7115 - val_loss: 0.3369 - val_accuracy: 0.9387\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.8648 - accuracy: 0.7154 - val_loss: 0.3241 - val_accuracy: 0.9397\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.8339 - accuracy: 0.7237 - val_loss: 0.3027 - val_accuracy: 0.9367\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7981 - accuracy: 0.7360 - val_loss: 0.2796 - val_accuracy: 0.9448\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7603 - accuracy: 0.7500 - val_loss: 0.2592 - val_accuracy: 0.9428\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7436 - accuracy: 0.7548 - val_loss: 0.2492 - val_accuracy: 0.9479\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7345 - accuracy: 0.7640 - val_loss: 0.2383 - val_accuracy: 0.9469\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.7771 - val_loss: 0.2272 - val_accuracy: 0.9499\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.7732 - val_loss: 0.2163 - val_accuracy: 0.9489\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.7842 - val_loss: 0.2067 - val_accuracy: 0.9499\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.7850 - val_loss: 0.1954 - val_accuracy: 0.9530\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6225 - accuracy: 0.7942 - val_loss: 0.1917 - val_accuracy: 0.9551\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5940 - accuracy: 0.8025 - val_loss: 0.1770 - val_accuracy: 0.9571\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.7990 - val_loss: 0.1711 - val_accuracy: 0.9591\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.7990 - val_loss: 0.1647 - val_accuracy: 0.9591\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5710 - accuracy: 0.7968 - val_loss: 0.1660 - val_accuracy: 0.9602\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5671 - accuracy: 0.8109 - val_loss: 0.1622 - val_accuracy: 0.9602\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5455 - accuracy: 0.8144 - val_loss: 0.1572 - val_accuracy: 0.9581\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5217 - accuracy: 0.8253 - val_loss: 0.1504 - val_accuracy: 0.9612\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.8336 - val_loss: 0.1433 - val_accuracy: 0.9612\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.8209 - val_loss: 0.1385 - val_accuracy: 0.9602\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.8279 - val_loss: 0.1372 - val_accuracy: 0.9612\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.8236 - val_loss: 0.1293 - val_accuracy: 0.9612\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.8354 - val_loss: 0.1275 - val_accuracy: 0.9602\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.8433 - val_loss: 0.1303 - val_accuracy: 0.9632\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4750 - accuracy: 0.8389 - val_loss: 0.1256 - val_accuracy: 0.9622\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8625 - val_loss: 0.1199 - val_accuracy: 0.9622\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4448 - accuracy: 0.8459 - val_loss: 0.1185 - val_accuracy: 0.9622\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.8525 - val_loss: 0.1148 - val_accuracy: 0.9622\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4394 - accuracy: 0.8393 - val_loss: 0.1129 - val_accuracy: 0.9632\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.8542 - val_loss: 0.1135 - val_accuracy: 0.9612\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8555 - val_loss: 0.1102 - val_accuracy: 0.9622\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4228 - accuracy: 0.8538 - val_loss: 0.1065 - val_accuracy: 0.9622\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8669 - val_loss: 0.1100 - val_accuracy: 0.9612\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8599 - val_loss: 0.1047 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8735 - val_loss: 0.1008 - val_accuracy: 0.9632\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8665 - val_loss: 0.0979 - val_accuracy: 0.9632\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8660 - val_loss: 0.0975 - val_accuracy: 0.9642\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8708 - val_loss: 0.0968 - val_accuracy: 0.9622\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8647 - val_loss: 0.0936 - val_accuracy: 0.9632\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8678 - val_loss: 0.0948 - val_accuracy: 0.9642\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8717 - val_loss: 0.0939 - val_accuracy: 0.9622\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8717 - val_loss: 0.0920 - val_accuracy: 0.9622\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3746 - accuracy: 0.8765 - val_loss: 0.0915 - val_accuracy: 0.9622\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8809 - val_loss: 0.0894 - val_accuracy: 0.9632\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.8844 - val_loss: 0.0905 - val_accuracy: 0.9622\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3424 - accuracy: 0.8835 - val_loss: 0.0835 - val_accuracy: 0.9632\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8796 - val_loss: 0.0850 - val_accuracy: 0.9632\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8901 - val_loss: 0.0823 - val_accuracy: 0.9632\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8831 - val_loss: 0.0839 - val_accuracy: 0.9642\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8875 - val_loss: 0.0814 - val_accuracy: 0.9632\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8844 - val_loss: 0.0799 - val_accuracy: 0.9632\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8879 - val_loss: 0.0787 - val_accuracy: 0.9632\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8866 - val_loss: 0.0785 - val_accuracy: 0.9622\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8936 - val_loss: 0.0753 - val_accuracy: 0.9653\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8796 - val_loss: 0.0775 - val_accuracy: 0.9632\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8971 - val_loss: 0.0773 - val_accuracy: 0.9622\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8901 - val_loss: 0.0772 - val_accuracy: 0.9632\n",
      "Epoch 81: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Creating a Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "# model 1\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(100, activation='softmax'))\n",
    "\n",
    "# Learning Process of a model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "#es = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "print(X_train)\n",
    "#Train with early stopping to avoid overfitting\n",
    "y_train=np.array(y_train, dtype=int)\n",
    "y_val=np.array(y_val, dtype=int)\n",
    "history = model.fit(X_train,y_train,validation_data=(X_val, y_val),epochs=100,batch_size=128,class_weight=weight, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeaker(speaker):\n",
    "    speaker = \"Speaker\"+str(speaker).zfill(3)\n",
    "    return speaker\n",
    "    \n",
    "        \n",
    "def printPrediction(X_data, y_data, printDigit, model):\n",
    "    print('\\n# Generate predictions')\n",
    "    for i in range(len(y_data)):\n",
    "        predict_x=model.predict(X_data[i:i+1])[0]\n",
    "        predict_classes = np.argmax(predict_x)\n",
    "        prediction = getSpeaker(predict_classes)\n",
    "    \n",
    "        speaker = getSpeaker(y_data[i])\n",
    "        if printDigit == True:\n",
    "           print(\"Number={0:d}, y={1:10s}- prediction={2:10s}- match={3}\".format(i, speaker, prediction, speaker==prediction))\n",
    "        else:\n",
    "           print(\"y={0:10s}- prediction={1:10s}- match={2}\".format(speaker, prediction, speaker==prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def report(X_data, y_data):\n",
    "    #Confution Matrix and Classification Report\n",
    "    predict_y = model.predict(X_data)\n",
    "    Y_pred = np.argmax(predict_y, axis=1)\n",
    "    y_test_num = y_data.astype(np.int64)\n",
    "    conf_mt = confusion_matrix(y_test_num, Y_pred)\n",
    "    print(conf_mt[6])\n",
    "    key=0\n",
    "    for val in conf_mt[6]:\n",
    "        if val!=0:\n",
    "            print(key)\n",
    "        key=key+1\n",
    "    conf_mt=conf_mt / conf_mt.astype(np.float).sum(axis=1)\n",
    "    #print(conf_mt)\n",
    "    plt.matshow(conf_mt)\n",
    "    plt.show()\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test_num, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# TEST DATA #\n",
      "\n",
      "27/27 [==============================] - 0s 753us/step - loss: 0.8307 - accuracy: 0.9344\n",
      "accuracy: 93.44%\n",
      "\n",
      "# Generate predictions\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "y=Speaker032- prediction=Speaker032- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker054- prediction=Speaker054- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker013- prediction=Speaker013- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker048- prediction=Speaker048- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker001- prediction=Speaker001- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker019- prediction=Speaker019- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker000- prediction=Speaker000- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker052- prediction=Speaker053- match=False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker044- prediction=Speaker044- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker030- prediction=Speaker030- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker054- prediction=Speaker054- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker009- prediction=Speaker009- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker002- prediction=Speaker002- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker018- prediction=Speaker018- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker045- prediction=Speaker045- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker055- prediction=Speaker055- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker016- prediction=Speaker016- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker009- prediction=Speaker009- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker000- prediction=Speaker000- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker015- prediction=Speaker015- match=True\n"
     ]
    }
   ],
   "source": [
    "print('\\n# TEST DATA #\\n')\n",
    "y_test=np.array(y_test, dtype=int)\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "# Prediction\n",
    "printPrediction(X_test[0:20], y_test[0:20], False, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Test Data\n",
      "\n",
      "27/27 [==============================] - 0s 700us/step\n",
      "[ 0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQG0lEQVR4nO3dfYxc9XXG8e/jXQN5wcEbiLVhIcubQmhFTOXyUlBLoARKIkAqQklR5UhWraYvgqRSwK0SNWkloH+ERFVLtA0orkQCFEKMKK3jOqYVbWJYwLwYQ22chdoYltQQIFJdr336x1y7ey+7O7Mzc+/c2d/zkVZzz52Xe8Tgs+ee/d0ZRQRmlq5FvU7AzHrLRcAscS4CZolzETBLnIuAWeJcBMwSV2kRkHSZpBck7ZB0Y5XHniWfOyRNSnp22r4hSRskbc9ul/YwvxMkbZL0nKStkq6rU46SjpL0qKSnsvy+mu0/SdLm7H2+W9IRvcgvy2VA0pOSHqxbblk+E5KekbRF0ni2r9L3t7IiIGkA+Bvgt4AzgM9KOqOq48/iO8BlhX03Ahsj4jRgYxb3yhTwJxFxBnAu8IfZf7O65LgPuCgiPg4sBy6TdC5wC3BrRJwKvAGs6lF+ANcB26bFdcrtkE9ExPKIWJHF1b6/EVHJD3AesH5avAZYU9Xx58hrFHh2WvwCMJxtDwMv9DrHabmtAy6pY47Ae4EngHOAnwGDM73vFec0kv0jugh4EFBdcpuW4wRwbGFfpe9vlacDxwP/NS3ele2rm2URsSfbfhVY1stkDpE0CpwFbKZGOWbt9hZgEtgAvAi8GRFT2UN6+T5/A/gScDCLP0h9cjskgB9KelzS6mxfpe/vYJkv3u8iIiT1fF21pPcD9wHXR8Rbkg7f1+scI+IAsFzSMcD9wOm9ymU6SZ8GJiPicUkX9jiduVwQEbslfQjYIOn56XdW8f5W2QnsBk6YFo9k++rmNUnDANntZC+TkbSYRgG4MyK+n+2uVY4AEfEmsIlGi32MpEO/YHr1Pp8PXCFpAriLxinBN2uS22ERsTu7naRRRM+m4ve3yiLwGHBaNp09AvgM8ECFx2/VA8DKbHsljfPwnlDjV/7twLaI+Pq0u2qRo6Tjsg4ASe+hMa/YRqMYXN3L/CJiTUSMRMQojf/XfhQR19Yht0MkvU/S0Ye2gU8Cz1L1+1vxEORy4D9pnDf+WS8HMlk+3wP2APtpnB+uonHeuBHYDvwLMNTD/C6gcc74NLAl+7m8LjkCZwJPZvk9C3wl238y8CiwA/gH4Mgev88XAg/WLbcsl6eyn62H/k1U/f4qO6iZJcorBs0S5yJgljgXAbPEuQiYJc5FwCxxPSkC05ZH1pLza1+dcwPnN5OOikAHlwbX+o3A+XWizrmB83uXtotATS8NNrN5anuxkKTzgD+PiEuzeA1ARNw023OGhhbF8SMD7N17kKGhRbz0zNFzH2Mwf31TTE3N8sju2s8+FnNkJcdqR53zq3NukG5+/8Mv+N/Yp5nu6+QqwpkuDT5nzieMDPCDfzz2cPz7H7lgzgMMDB2Xiw+8/vp8czQzYHNsnPW+0geDklZLGpc0vnfvweZPMLNKddIJtHRpcESMAWMASzQU03/7r39lS+6xl354eS72b36z8nXSCfTLpcFmNoe2O4GImJL0R8B6YAC4IyK2di0zM6tERx8vFhEPAQ91KRcz64GefsZgcQbwnZcfycWfO3Huvx5YwaKBfHzwQG/ysL7iawfMEuciYJY4FwGzxNXqeweKM4C/funfc/Eff+T8KtMxS4I7AbPEuQiYJc5FwCxxtZoJFF2/4qpc/Lcv3Z+L/6DJVYjJ8boAa4M7AbPEuQiYJc5FwCxxtZ4JFD9PoDgDaPZ5BGbWnDsBs8S5CJglzkXALHG1mgn8/KFTc/EHLt8x5+M9AzDrnDsBs8S5CJglzkXALHG1mgk0mwHMl9cRmDXnTsAscS4CZolzETBLXK1mAt1WnAF4RmD2bu4EzBLnImCWOBcBs8Qt6JlAUXEGcMtPN+fiG046p8JsSuDvIrQ2uBMwS5yLgFnimhYBSXdImpT07LR9Q5I2SNqe3S4tN00zK4siYu4HSL8OvAP8fUT8crbvr4C9EXGzpBuBpRFxQ7ODLdFQnKOLu5B2ObyOwBaqzbGRt2KvZrqvaScQEf8G7C3svhJYm22vBa7qJEEz6512ZwLLImJPtv0qsKxL+ZhZxToeDEbjfGLWcwpJqyWNSxrfz75OD2dmXdbuOoHXJA1HxB5Jw8DkbA+MiDFgDBozgTaPV4my1xEMnjyai6d2TnT0embd0G4n8ACwMtteCazrTjpmVrVW/kT4PeDHwEcl7ZK0CrgZuETSduA3s9jM+lDT04GI+Owsd9X3b31m1rKkrh2Yr+IM4Nrnd+XiO08fmdfreQbQXYOjJ+biqYmXe5RJf/OyYbPEuQiYJc5FwCxxngnMQ3EG8Pnt+e9JuO20/HcpVi6xzxPwDKA73AmYJc5FwCxxLgJmifNMoAPFGUDPP49ggc8ArBzuBMwS5yJgljgXAbPEeSbQRcUZQO3WESwwvnagO9wJmCXORcAscS4CZonzTKBEtVtHsMB4BtAd7gTMEuciYJY4FwGzxHkmUKHiDMAzAqsDdwJmiXMRMEuci4BZ4jwT6KGuX2uQ2GcMWne4EzBLnIuAWeJcBMwS55lAjXR8rYFnANYGdwJmiXMRMEtc0yIg6QRJmyQ9J2mrpOuy/UOSNkjant0uLT9dM+s2RcTcD5CGgeGIeELS0cDjwFXA54C9EXGzpBuBpRFxw1yvtURDcY4u7kriZkVv/c65uXjJd3/So0zqZ3Ns5K3Yq5nua9oJRMSeiHgi234b2AYcD1wJrM0etpZGYTCzPjOvmYCkUeAsYDOwLCL2ZHe9CizrbmpmVoWWi4Ck9wP3AddHxFvT74vGOcWM5xWSVksalzS+n30dJWtm3dfSOgFJi2kUgDsj4vvZ7tckDUfEnmxuMDnTcyNiDBiDxkygCzlb5i9++lgu/vIp+XPi1NYNeAbQnlb+OiDgdmBbRHx92l0PACuz7ZXAuu6nZ2Zla6UTOB/4XeAZSVuyfX8K3AzcI2kV8BJwTSkZmlmpmhaBiHgEmPFPC4D/3mfW53ztQB/78km/mov/7uV/zcW/d+IFVaZjPTawNL9e78Abb7T0PC8bNkuci4BZ4lwEzBLnmUCFBk8ezcVTOye6+vrFGUDHn1lofaXVGUCROwGzxLkImCXORcAscZ4JdNHg6Im5eGri5Xzc5RlAM8UZwFd2PpGLv3byr1SZjtWUOwGzxLkImCXORcAscZ4JJKQ4A1jz4tO5+KZTzqwyHasJdwJmiXMRMEuci4BZ4jwT6KLiuoC6K84AvrBjWy6+9dSPVZlO5RYtPyMXH9zyXI8y6S13AmaJcxEwS5yLgFniPBNYSBYN5ON5fu9AcQaw0NcRpDoDKHInYJY4FwGzxLkImCXOM4GFpMvfPVicAax/ZUsuvvTDy7t6POsNdwJmiXMRMEuci4BZ4jwTsJYVZwCeESwM7gTMEuciYJa4pkVA0lGSHpX0lKStkr6a7T9J0mZJOyTdLemI8tM1s25rZSawD7goIt6RtBh4RNI/AV8Ebo2IuyR9C1gF3FZirlYzxRnAtc/vysV3nj5SYTbz588TaGjaCUTDO1m4OPsJ4CLg3mz/WuCqMhI0s3K1NBOQNCBpCzAJbABeBN6MiKnsIbuA40vJ0MxK1VIRiIgDEbEcGAHOBk5v9QCSVksalzS+n33tZWlmpZnXOoGIeFPSJuA84BhJg1k3MALsnuU5Y8AYwBINRYf5Wo0VZwB1X0eQ6gygqJW/Dhwn6Zhs+z3AJcA2YBNwdfawlcC6knI0sxK10gkMA2slDdAoGvdExIOSngPukvSXwJPA7SXmaWYlaVoEIuJp4KwZ9u+kMR8wsz7mawesNL7WoD942bBZ4lwEzBLnImCWOM8ErDKeEdSTOwGzxLkImCXORcAscZ4JWM94RlAP7gTMEuciYJY4FwGzxHkmYLXhGUFvuBMwS5yLgFniXATMEueZQBcNjp6Yi6cmXu5RJgtDv3+vQb9wJ2CWOBcBs8S5CJglzjOBLvIMoFz99r0G/cKdgFniXATMEuciYJY4zwSsb33q167Ixdc+/x+52OsIWuNOwCxxLgJmiXMRMEucZwLWt4rrMryOoD3uBMwS5yJglriWi4CkAUlPSnowi0+StFnSDkl3SzqivDTNrCzzmQlcB2wDlmTxLcCtEXGXpG8Bq4DbupyfWdv8mYWtaakTkDQCfAr4dhYLuAi4N3vIWuCqEvIzs5K1ejrwDeBLwMEs/iDwZkRMZfEu4PjupmZmVWhaBCR9GpiMiMfbOYCk1ZLGJY3vZ187L2FmJWplJnA+cIWky4GjaMwEvgkcI2kw6wZGgN0zPTkixoAxgCUaiq5kbZUYPHk0F0/tnOjo9QZ+6aO5+MDWFzp6vfkqzgAWPzyci/dfuKfCbOqjaScQEWsiYiQiRoHPAD+KiGuBTcDV2cNWAutKy9LMStPJOoEbgC9K2kFjRnB7d1IysyrNa9lwRDwMPJxt7wTO7n5KZlYlXztgs+p0BvCumULFM4BmijOAL+zYlotvPfVjVabTM142bJY4FwGzxLkImCXOMwErTaczhWa6/d2PxRlAKt996E7ALHEuAmaJcxEwS5xnAmazKM4APr99Ry6+7bRTq0ynNO4EzBLnImCWOBcBs8R5JmB9q9N1AfNVnAEslM8sdCdgljgXAbPEuQiYJc4zAbM2LZTvNXAnYJY4FwGzxLkImCXOMwFL1qLlZ+Tig1ue6+j1ijOAfvk8AncCZolzETBLnIuAWeI8E1hIFg3k44MHepNHn+h0BtBMcQZQ1xmBOwGzxLkImCXORcAscZ4JLCSJzQC6/b0DZSvOANa8+HQuvumUM6tM5zB3AmaJa6kTkDQBvA0cAKYiYoWkIeBuYBSYAK6JiDfKSdPMyjKfTuATEbE8IlZk8Y3Axog4DdiYxWbWZzqZCVwJXJhtrwUeBm7oMB+zljWbAdR9ZlCcAfRqHUGrnUAAP5T0uKTV2b5lEbEn234VWNb17MysdK12AhdExG5JHwI2SHp++p0REZJipidmRWM1wFG8t6Nkzaz7WuoEImJ3djsJ3A+cDbwmaRggu52c5bljEbEiIlYs5sjuZG1mXdO0E5D0PmBRRLydbX8S+BrwALASuDm7XVdmombzVbcZQDPFGUBVn1nYyunAMuB+SYce/92I+GdJjwH3SFoFvARcU0qGZlaqpkUgInYCH59h/38DF5eRlJlVxysGzRLnawfMaqo4A/j5Q/nvQtz/g+Ny8bFjP27rOO4EzBLnImCWOBcBs8R5JmDWJz5w+Y5cvP6Ve3PxpWPL23pddwJmiXMRMEuci4BZ4jwTMOtTxXUE9+36SS6++pTf+P9gn2Z9HXcCZolzETBLnIuAWeI8EzBbIH575NxcvP6VzYe3z770F7M+z52AWeJcBMwS5yJgljhFzPghweUcTHqdxkeRHQv8rLIDz5/za1+dc4N08/tIRBw30x2VFoHDB5XGp32TUe04v/bVOTdwfjPx6YBZ4lwEzBLXqyIw1qPjtsr5ta/OuYHze5eezATMrD58OmCWOBcBs8S5CJglzkXALHEuAmaJ+z99RXODJkZzmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        19\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       0.59      0.96      0.73        24\n",
      "           3       0.89      0.94      0.91        17\n",
      "           4       1.00      0.94      0.97        18\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       0.00      0.00      0.00        16\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        20\n",
      "           9       1.00      0.92      0.96        13\n",
      "          10       1.00      1.00      1.00        11\n",
      "          11       1.00      0.92      0.96        13\n",
      "          12       1.00      0.86      0.92         7\n",
      "          13       1.00      0.88      0.93         8\n",
      "          14       0.79      1.00      0.88        11\n",
      "          15       1.00      0.88      0.93         8\n",
      "          16       0.83      1.00      0.91        10\n",
      "          17       1.00      1.00      1.00         9\n",
      "          18       1.00      0.88      0.93         8\n",
      "          19       1.00      1.00      1.00         7\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       1.00      0.92      0.96        12\n",
      "          22       0.92      0.95      0.94       100\n",
      "          23       1.00      0.88      0.93         8\n",
      "          24       1.00      0.91      0.95        11\n",
      "          25       1.00      0.90      0.95        10\n",
      "          26       1.00      0.89      0.94         9\n",
      "          27       0.90      0.90      0.90        10\n",
      "          28       1.00      1.00      1.00        12\n",
      "          29       1.00      1.00      1.00         7\n",
      "          30       1.00      0.86      0.92         7\n",
      "          31       0.91      1.00      0.95        10\n",
      "          32       0.89      1.00      0.94         8\n",
      "          33       1.00      1.00      1.00         7\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      1.00      1.00         7\n",
      "          36       1.00      0.86      0.92         7\n",
      "          37       0.65      1.00      0.79        11\n",
      "          38       0.75      0.86      0.80         7\n",
      "          39       0.92      1.00      0.96        11\n",
      "          40       1.00      1.00      1.00         7\n",
      "          41       0.62      0.71      0.67         7\n",
      "          42       1.00      0.89      0.94         9\n",
      "          43       1.00      0.86      0.92         7\n",
      "          44       1.00      0.88      0.93         8\n",
      "          45       1.00      1.00      1.00         9\n",
      "          46       1.00      1.00      1.00         8\n",
      "          47       1.00      0.86      0.92         7\n",
      "          48       1.00      0.86      0.92         7\n",
      "          49       1.00      0.90      0.95        10\n",
      "          50       1.00      0.86      0.92         7\n",
      "          51       1.00      1.00      1.00         6\n",
      "          52       1.00      0.67      0.80         6\n",
      "          53       0.75      1.00      0.86         6\n",
      "          54       0.95      0.98      0.97       100\n",
      "          55       0.98      1.00      0.99       100\n",
      "\n",
      "    accuracy                           0.93       838\n",
      "   macro avg       0.92      0.90      0.90       838\n",
      "weighted avg       0.93      0.93      0.93       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for Test Data\\n\")\n",
    "report(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('speaker-recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transfer learning\n",
    "\n",
    "https://keras.io/guides/transfer_learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Generate predictions\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "y=Speaker032- prediction=Speaker032- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker054- prediction=Speaker054- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker013- prediction=Speaker013- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker048- prediction=Speaker048- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker001- prediction=Speaker001- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker019- prediction=Speaker019- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker000- prediction=Speaker000- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker052- prediction=Speaker053- match=False\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker044- prediction=Speaker044- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker030- prediction=Speaker030- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker054- prediction=Speaker054- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker009- prediction=Speaker009- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker002- prediction=Speaker002- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker018- prediction=Speaker018- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker045- prediction=Speaker045- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker055- prediction=Speaker055- match=True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "y=Speaker016- prediction=Speaker016- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker009- prediction=Speaker009- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker000- prediction=Speaker000- match=True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "y=Speaker015- prediction=Speaker015- match=True\n"
     ]
    }
   ],
   "source": [
    "new_model = keras.models.load_model('speaker-recognition.h5')\n",
    "printPrediction(X_test[0:20], y_test[0:20], False, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the files in the folder audios/audios/one_test will be saved to final_test.csv\n",
      "CSV Header:  ['filename', 'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of extractWavFeatures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#filename='Speaker0055_477.wav'\n",
    "extractWavFeatures(\"audios/audios/one_test\", final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test.csv will be preprocessed\n",
      "Preprocessing is finished\n",
      "       rmse  spectral_centroid  spectral_bandwidth      rolloff  \\\n",
      "0  0.039182        1029.761278         1032.513716  2147.338867   \n",
      "\n",
      "   zero_crossing_rate       mfcc1       mfcc2      mfcc3      mfcc4  \\\n",
      "0            0.053385 -363.783813  223.351669 -71.833092  21.587334   \n",
      "\n",
      "       mfcc5  ...     mfcc12    mfcc13    mfcc14     mfcc15     mfcc16  \\\n",
      "0  37.874554  ... -22.200153  7.744421 -1.612798 -15.068378  11.225948   \n",
      "\n",
      "     mfcc17     mfcc18    mfcc19    mfcc20  number  \n",
      "0 -1.377748 -12.102902  7.221355 -4.805168      55  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "final_testData = preProcessData(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y from test data: (1,)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(final_testData.iloc[:, :-1], dtype = float)\n",
    "y_test = final_testData.iloc[:, -1]\n",
    "print(\"Y from test data:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3421/3542434633.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_finaltest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X from test data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_finaltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_finaltest = scaler.transform( X_test )\n",
    "\n",
    "print(\"X from test data\", X_finaltest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
